{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d192a0-ea2d-4363-a74d-9098c0ccbe15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Description\n",
    "_Mechanism Y starts at the same time as X and ingests the S3 stream as \n",
    "soon as transaction chunk files become available, detects the below patterns asap and puts \n",
    "these detections to S3 , 50 at a time to a unique file._\n",
    "\n",
    "_**Patterns:**_\n",
    "\n",
    "_**PatId1** - A customer in the top 1 percentile for a given merchant for the total number of \n",
    "transactions with the bottom 1% percentile weight, merchant wants to UPGRADE(actionType) \n",
    "them. Upgradation only begins once total transactions for the merchant exceed 50K._\n",
    "\n",
    "_**PatId2** - A customer whose average transaction value for a given merchant < Rs 23 and made \n",
    "at least 80 transactions with that merchant, merchant wants to mark them as CHILD(actionType) asap._\n",
    "\n",
    "_**PatId3**- Merchants where number of Female customers < number of Male customers overall \n",
    "and number of female customers > 100, are marked DEI-NEEDED(actionType)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393fc7db-c3bc-4d82-b4bd-541dc8a2ec69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426d1643-f57a-4bce-a8de-03793683388c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "spark = SparkSession.builder.\\\n",
    "    appName(\"Batch_MechanismY\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Paths\n",
    "customer_s3_path = \"s3://customer-transactions-detection/input/customers/\"\n",
    "transaction_s3_path = \"s3://customer-transactions-detection/input/transactions/\"\n",
    "stage_path = \"/Volumes/workspace/devdolphins/staging/merchant/\"\n",
    "detection_path = \"s3://customer-transactions-detection/output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c6f39a-0c49-43f5-a131-546408d01d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Defining schemas for customerImportance and Transactions DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecced21-c9df-4394-90d3-7c97c7fc99de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition\n",
    "customer_importance_schema = StructType([\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"typeTrans\", StringType(), True),\n",
    "    StructField(\"fraud\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"step\", IntegerType()),\n",
    "    StructField(\"customer\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"zipcodeOri\", StringType()),\n",
    "    StructField(\"merchant\", StringType()),\n",
    "    StructField(\"zipMerchant\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"fraud\", IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ee47fa-1cac-4bc4-87be-c84b8e907ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load the CustomerImportance file from S3 bucket and performing aggregations to get the total weight and frauds per consumer,merchant and transaction type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652e212b-7564-4057-906f-2a945f73d435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read customer importance data\n",
    "customer_df = spark.read.csv(customer_s3_path, header=True, schema=customer_importance_schema)\n",
    "customer_df = broadcast(customer_df)\n",
    "\n",
    "agg_customer_df = customer_df.groupBy(\"customer\", \"merchant\") \\\n",
    "    .agg(sum(\"weight\").alias(\"total_weight\"), sum(\"fraud\").alias(\"fraud_rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "028614a1-ea64-44ac-8978-fcd8acbe0925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**process_chunk** function performs detections and write the output to S3 bucket for each chunk received in the transactions S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "661fa137-ef5d-4c6e-9475-4b6fc9e43137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_chunk(chunk_path, batch_id):\n",
    "    print(f\"Processing {chunk_path}\")\n",
    "    process_time = datetime.now().isoformat()\n",
    "    \n",
    "    # Read transaction chunk\n",
    "    df = spark.read.csv(chunk_path, header=True, schema=transaction_schema) \\\n",
    "            .withColumn(\"YStartTime\", lit(process_time))\n",
    "    \n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .save(stage_path)\n",
    "\n",
    "    stage_df = spark.read.format(\"delta\").load(stage_path)\n",
    "    # Precompute merchant and customer information\n",
    "    merchant_txn = stage_df.groupBy(\"merchant\").agg(\n",
    "        sum(\"amount\").alias(\"transactions\"),\n",
    "        countDistinct(when(col(\"gender\") == \"'F'\", col(\"customer\"))).alias(\"no_of_female_customers\"),\n",
    "        countDistinct(when(col(\"gender\") == \"'M'\", col(\"customer\"))).alias(\"no_of_male_customers\")\n",
    "        )\n",
    "    #merchant_txn.cache()\n",
    "\n",
    "    customer_txn = stage_df.groupBy(\"customer\", \"merchant\").agg(\n",
    "    count(\"*\").alias(\"txn_count\"), avg(\"amount\").alias(\"avg_txn\")\n",
    "        )\n",
    "    #customer_txn.cache()\n",
    "    # Pattern 1\n",
    "    merchant_txn_pat1 = merchant_txn.filter(col(\"transactions\") > 50000)\n",
    "    pattern1 = df.join(merchant_txn_pat1, \"merchant\")\\\n",
    "        .join(customer_txn, [\"customer\", \"merchant\"]) \\\n",
    "        .join(agg_customer_df, [\"customer\", \"merchant\"])\\\n",
    "        .withColumn(\"rank\", percent_rank().over(Window.partitionBy(\"customer\", \"merchant\").orderBy(\"txn_count\"))) \\\n",
    "        .withColumn(\"low_weight_rank\", percent_rank().over(Window.partitionBy(\"customer\", \"merchant\").orderBy(\"total_weight\"))) \\\n",
    "        .filter((col(\"rank\") >= 0.99) & (col(\"low_weight_rank\") <= 0.01)) \\\n",
    "        .withColumn(\"ActionType\", lit(\"UPGRADE\")) \\\n",
    "        .withColumn(\"patternId\", lit(\"PatId1\"))\n",
    "\n",
    "    pat1_formatted = pattern1.withColumn(\"customerName\", col(\"customer\")) \\\n",
    "        .withColumn(\"MerchantId\", col(\"merchant\")) \\\n",
    "        .withColumn(\"ActionType\", lit(\"UPGRADE\")) \\\n",
    "        .withColumn(\"patternId\", lit(\"PatId1\")) \\\n",
    "        .withColumn(\"detectionTime\", lit(datetime.now().isoformat())) \\\n",
    "        .select(\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"MerchantId\")\n",
    "\n",
    "    # Pattern 2\n",
    "    pattern2 = df.join(customer_txn, [\"customer\", \"merchant\"]) \\\n",
    "        .filter((col(\"avg_txn\") < 23) & (col(\"txn_count\") >= 80))\n",
    "\n",
    "    pat2_formatted = pattern2.withColumn(\"customerName\", col(\"customer\")) \\\n",
    "        .withColumn(\"MerchantId\", col(\"merchant\")) \\\n",
    "        .withColumn(\"ActionType\", lit(\"CHILD\")) \\\n",
    "        .withColumn(\"patternId\", lit(\"PatId2\")) \\\n",
    "        .withColumn(\"detectionTime\", lit(datetime.now().isoformat())) \\\n",
    "        .select(\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"MerchantId\")\n",
    "    \n",
    "    # Pattern 3\n",
    "    pattern3 = df.join(merchant_txn, \"merchant\") \\\n",
    "        .filter((col(\"no_of_female_customers\") > 100) & (col(\"no_of_female_customers\") < col(\"no_of_male_customers\")))\n",
    "\n",
    "    pat3_formatted = pattern3.withColumn(\"customerName\", lit(\"\")) \\\n",
    "        .withColumn(\"MerchantId\", col(\"merchant\")) \\\n",
    "        .withColumn(\"ActionType\", lit(\"DEI-NEEDED\")) \\\n",
    "        .withColumn(\"patternId\", lit(\"PatId3\")) \\\n",
    "        .withColumn(\"detectionTime\", lit(datetime.now().isoformat())) \\\n",
    "        .select(\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"MerchantId\")\n",
    "\n",
    "    all_detections = pat1_formatted.unionByName(pat2_formatted).unionByName(pat3_formatted)\n",
    "\n",
    "    # Chunk detections in 50-record batches\n",
    "    row_window = Window.orderBy(\"patternId\", \"YStartTime\")\n",
    "    all_detections = all_detections.withColumn(\"row_num\", row_number().over(row_window))\n",
    "    all_detections = all_detections.withColumn(\"group_id\", floor((col(\"row_num\") - 1) / 50))\n",
    "\n",
    "    # Get distinct group_id using DataFrame only\n",
    "    group_ids = [row['group_id'] for row in all_detections.select(\"group_id\").distinct().collect()]\n",
    "\n",
    "    for group in group_ids:\n",
    "        chunk_df = all_detections.filter(col(\"group_id\") == group).drop(\"row_num\", \"group_id\")\n",
    "        filename = f\"detection{batch_id}_group{group}.csv\"\n",
    "        output_path = os.path.join(detection_path, filename)\n",
    "        chunk_df.write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "        print(f\"Wrote detection chunk to {output_path}\")\n",
    "    \n",
    "    #merchant_txn.unpersist()\n",
    "    #customer_txn.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb35dbfa-2c07-41ed-8d7c-f1ce219b3149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Stream the transaction chunk from S3 bucket based on the arrival of the chunk and perform the pattern detections. The below code cell calls **process_chunk** function to perform detections and write the output to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a44c6c2-d874-43eb-8d6c-fd962ea70016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_id = 0\n",
    "no_new_chunk_seconds = 0\n",
    "TIMEOUT_LIMIT = 60\n",
    "# Track processed chunks\n",
    "processed_chunks = set()\n",
    "\n",
    "while True:\n",
    "    chunk_dirs = sorted([d.name for d in dbutils.fs.ls(transaction_s3_path) if d.name.startswith(\"chunk_\")])\n",
    "    new_chunks = [d for d in chunk_dirs if d not in processed_chunks]\n",
    "\n",
    "    if new_chunks:\n",
    "        for chunk in new_chunks:\n",
    "            chunk_path = os.path.join(transaction_s3_path, chunk)\n",
    "            print(chunk_path)\n",
    "            process_chunk(chunk_path, batch_id)\n",
    "            processed_chunks.add(chunk)\n",
    "            batch_id += 1\n",
    "        no_new_chunk_seconds = 0\n",
    "    else:\n",
    "        no_new_chunk_seconds += 1\n",
    "        print(f\"No new chunks... waiting ({no_new_chunk_seconds}/60)\")\n",
    "        if no_new_chunk_seconds >= TIMEOUT_LIMIT:\n",
    "            print(\"No new chunks for 60 seconds. Terminating gracefully.\")\n",
    "            break\n",
    "\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism-Y",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}