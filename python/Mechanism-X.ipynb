{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db74fdc8-936c-45a1-bee9-74dce72f5c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Description\n",
    "_Mechanism X puts `CustomerImportance.csv` at once and creates a chunk of 10,000 transaction entries from `transactions.csv` every 1 second and puts them into an S3 folder._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5158d10-eb32-44f6-a61f-c6aeb77e6015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, row_number, monotonically_increasing_id, floor\n",
    "\n",
    "# Creating SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"Mechanism_X\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18b5c9e-9b12-4e53-9be2-5ee23eb61543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingesting CustomerImportance.csv to S3 Bucket\n",
    "customer_input_path = \"/Volumes/workspace/devdolphins/source/CustomerImportance.csv\"\n",
    "#customer_s3_path = \"/Volumes/workspace/devdolphins/output/customer/\"\n",
    "customer_s3_path = \"s3://customer-transactions-detection/input/customers/\"\n",
    "\n",
    "customer_importance_schema = StructType([\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"typeTrans\", StringType(), True),\n",
    "    StructField(\"fraud\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "customer_importance_df = spark.read.csv(customer_input_path, header=True, schema=customer_importance_schema)\n",
    "\n",
    "# Writing data to S3 bucket\n",
    "customer_importance_df.write.format(\"csv\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"header\",True)\\\n",
    "        .save(customer_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15d8203-c88c-47e4-af5c-b6b2edf683fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingesting next 10000 records from transactions.csv to S3 Bucket every 1 second\n",
    "transaction_input_path = \"/Volumes/workspace/devdolphins/source/transactions.csv\"\n",
    "#transaction_s3_path = \"/Volumes/workspace/devdolphins/output/transactions/\"\n",
    "transaction_s3_path = \"s3://customer-transactions-detection/input/transactions/\"\n",
    "chunk_size = 10000\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"step\", IntegerType()),\n",
    "    StructField(\"customer\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"zipcodeOri\", StringType()),\n",
    "    StructField(\"merchant\", StringType()),\n",
    "    StructField(\"zipMerchant\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"fraud\", IntegerType()),\n",
    "])\n",
    "\n",
    "transaction_df = spark.read.csv(transaction_input_path, header=True, schema=transaction_schema)\n",
    "\n",
    "# Add row_number() for accurate chunking\n",
    "windowSpec = Window.orderBy(\"step\")  # or any other column that makes sense for ordering\n",
    "transaction_df = transaction_df.withColumn(\"row_num\", row_number().over(windowSpec) - 1)\n",
    "transaction_df = transaction_df.withColumn(\"chunk_id\", floor(col(\"row_num\") / chunk_size))\n",
    "\n",
    "# Get total number of chunks\n",
    "total_chunks = transaction_df.select(\"chunk_id\").distinct().count()\n",
    "\n",
    "# Write each chunk\n",
    "for i in range(10):\n",
    "    chunk_path = os.path.join(transaction_s3_path, f\"chunk_{i:04d}\")\n",
    "    chunk_df = transaction_df.filter(col(\"chunk_id\") == i).drop(\"row_num\", \"chunk_id\")\n",
    "    \n",
    "    if chunk_df.count() == 0:\n",
    "        break\n",
    "    \n",
    "    print(f\"{chunk_path}: {chunk_df.count()}\")\n",
    "    \n",
    "    chunk_df.write.format(\"csv\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"header\", True)\\\n",
    "        .save(chunk_path)\n",
    "    \n",
    "    time.sleep(1)  # Simulate 1-second delay'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism-X",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}